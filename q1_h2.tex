\documentclass[11pt]{article}

\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{fullpage}

\usepackage{graphicx}
\graphicspath{ {./images/} }

\title{Math 207 HW2}
\author{Sam Craig}
\theoremstyle{definition}
% All theorems are numbered
\newtheorem{innercustomex}{Exercise}
\newenvironment{customex}[1]
  {\renewcommand\theinnercustomex{#1}\innercustomex}
  {\endinnercustomex}
\newtheorem*{solution}{Solution}

\usepackage{etoolbox}
\AtBeginEnvironment{align}{\setcounter{equation}{0}}

\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}

\DeclareMathOperator{\spn}{span}
\DeclareMathOperator{\nul}{null}
\DeclareMathOperator{\rank}{rank}
%ker already defined
\DeclareMathOperator{\im}{Im}
\DeclareMathOperator{\vdim}{dim}

\newcommand{\e}{\epsilon}
\newcommand{\de}{\delta}

\newtheorem*{lemma}{Lemma}

\renewcommand{\vec}[1]{\mathbf{#1}}
\newcommand{\va}{\mathbf{a}}
\newcommand{\vb}{\mathbf{b}}
\newcommand{\vp}{\mathbf{p}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\vu}{\mathbf{u}}
\newcommand{\vw}{\mathbf{w}}
\newcommand{\vx}{\mathbf{x}}
\newcommand{\vy}{\mathbf{y}}
\newcommand{\vz}{\mathbf{z}}
\newcommand{\vzero}{\mathbf{0}}
\newcommand{\Lagr}{\mathcal{L}}
\newcommand{\poly}{\mathcal{P}}

\begin{document}
    \maketitle

    \begin{customex}{5.1.5a and c}
        Explain why the following are not inner products on the given vector space.
        \begin{enumerate}[label = \alph*)]
            \item $\vx \cdot \vy = x_1y_1 - x_2y_2$ over $\R^2$.
            \item Skip
            \item $f \cdot g = \int_0^1 f'(t)g(t)dt$ over the space of polynomials
        \end{enumerate}
    \end{customex}

    \begin{solution}
        \begin{enumerate}[label = \alph*)]
            \item Let $\vx = (1,1)$. Then $\vx \cdot \vx \cdot \vx = 1 - 1 = 0$ but $\vx \neq 0$. Therefore $\cdot$ is not an inner product.
            \item Skip
            \item Let $f = x - \frac{1}{2}$. Then $f'(x) = 1$. $f \cdot f = \int_0^1 f'(t)f(t) dt = \int_0^1 f(t) dt = 0$, but $f \neq 0$. Therefore $\cdot$ is not an inner product. $\cdot$ also isn't commutative.
        \end{enumerate}
    \end{solution}
    
    \begin{customex}{5.1.9}
        Consider the space $\R^2$ with the norm $| \cdot |_p$, introduced in Section 1.5. For
        $p = 1,2,\inf$ draw the “unit ball” $B_p$ in the norm $| \cdot |_p$ 
            $$B_p = \{\vx \in \R^2 : |\vx|_p < 1\}$$
        Can you guess what the balls $B_p$ for other $p$ look like?
    \end{customex}

    \begin{solution}
       ~\\\includegraphics{B_1.png}
        \includegraphics{B_2.png}
        \includegraphics{B_inf.png}\\
        As $p$ increases, the ball becomes less circular, with a higher slope near $|x| = 1$ and $|y| = 1$ and turning more sharply around the line $y = \frac{x}{2}$ and $y = - \frac{x}{2}$.
    \end{solution}

    \begin{customex}{5.3.4}
        Find the distance from a vector $(2,3,1)^T$ to the subspace spanned by the vectors $(1, 2, 3)^T , (1, 3, 1)^T$.
    \end{customex} 

    \begin{solution}
        We will let $W = \spn (1, 2, 3), (1, 3, 1)$. Then, by the cross product we find $(-7,2,1) \perp (1, 2, 3)$ and $(-7,2,1) \perp  (1, 3, 1)$, so $W^{\perp} = \spn(-7,2,1)$. Then by exercise 5.2, $(2,3,1) = \alpha(1,2,3) + \beta(1,3,1) + \gamma(-7,2,1)$. Solving the system of equations gives $\alpha = \frac{1}{54}, \beta = \frac{29}{27}, \gamma = \frac{7}{54}$. By 5.3, the closest point in $W$ to $(2,3,1)$ is $\alpha(1,2,3) + \beta(1,3,1) = (\frac{59}{54},\frac{88}{27}, \frac{61}{54})$. The distance from $(2,3,1)$ to $(\frac{59}{54},\frac{88}{27}, \frac{61}{54})$ is $\frac{7}{3\sqrt{6}}$.
    \end{solution}

    \begin{customex}{5.3.13}
        Suppose $P$ is the orthogonal projection onto a subspace $E$, and $Q$ is the orthogonal projection onto the orthogonal complement $E^{\perp}$.
        \begin{enumerate}[label=\alph*)]
            \item What are $P + Q$ and $PQ$?
            \item Show that $P - Q$ is its own inverse.
        \end{enumerate}
    \end{customex}

    \begin{solution}
        \begin{enumerate}
            \item Let $\vv \in V$. Then $\vv = \vw + \vw^{\perp}$ for $\vw \in E$, $\vw^{\perp} \in E^{\perp}$. Then $P(\vv) = \vw$ and $Q(\vv) = \vw^{\perp}$. Thus, $P+Q(\vv) = \vw + \vw^{\perp} = \vv$, so $P+Q = I$. For any $\vv \in V$, $Q(\vv)  = \vw^{\perp}\in E^{\perp}$. Since $\vw^{\perp} \in E^{\perp}$, its orthogonal projection in $E$ is $\vzero$, so $PQ(\vv) = \vzero$ and thus $PQ = \vzero$.
            \item Let $\vv \in V$. By part a), $P-Q(\vv) = \vw - \vw^{\perp}$ with $\vw \in E$ and $\vw^{\perp} \in E^{\perp}$. Clearly $P(\vw - \vw^{\perp}) = \vw$ and $Q(\vw - \vw^{\perp}) = -\vw^{\perp}$, so $P-Q(\vw - \vw^{\perp}) = \vw + \vw^{\perp} = \vv$. Thus $(P-Q) \circ (P-Q) = I$, so $(P-Q)^{-1} = P-Q$. 
        \end{enumerate}
    \end{solution}

    \begin{customex}{\textbf{0}}
        Consider a linear map $T:\R^n \to \R^n$.
        \begin{enumerate}[label=\alph*)]
            \item Show that $T$ is continuous.
            \item  Show that if there is a nonempty open set $O$ so that if $T(O)$ is open, then $T(U)$ is open for every open
            set $U$.
            \item Show that $T$ is bijective if and only if there is a nonempty open set $O$ whose image $T(O)$ is an open set.
        \end{enumerate}
    \end{customex}

    \begin{solution}
        \begin{enumerate}[label=\alph*)]
            \item Set $$\alpha = \max_{i \le n, j \le n} (Te_i) \cdot e_j$$Fix $\e > 0$ and $\de = \frac{\e}{|\alpha|\sqrt{n}}$. Let $\vw \in \R^n$ such that $|\vw| < \de$. Then $\sqrt{n}|\alpha\vw| < \e$.$$\vw = \sum_{i = 1}^n \alpha_i e_i$$so$$T\vw =  \sum_{i = 1}^n \alpha_i Te_i$$which implies$$T\vw \cdot T\vw = \sum_{i = 1}^n (\alpha_i Te_i)^2 = \sum_{i = 1}^n \alpha_i^2 \left(\sum_{j = 1}^n (Te_i\cdot e_j)^2\right)$$since $T(e_i)\cdot e_j \le \alpha$, we have $$(T\vw)^2 = \sum_{i = 1}^n \alpha_i^2 \left(\sum_{j = 1}^n (Te_i\cdot e_j)^2\right) \le \sum_{i = 1}^n \alpha_i^2 n \alpha^2 = n\alpha^2\vw^2$$Then we have $$|T\vw| \le \sqrt{n}|\alpha\vw| < \e$$so $|T\vw| < \e$, so $\lim_{\vw \to \vzero} T\vw = \vzero$.
            
            Fix $\vv \in \R^n$. Then $$\lim_{\vw \to \vzero} T(\vv + \vw) = \lim_{\vw \to \vzero} T(\vv) + \lim_{\vw \to \vzero} T(\vw) = T(\vv) + \vzero = T(\vv)$$Since $\lim_{\vw \to \vzero} T(\vv + \vw) = T(\vv)$, $T$ is continuous.


            \item Let $U$ be an open subset of $\R^n$. Let $\vv \in U$ and $$B(\vv, \de) = \{\vx \in \R^n : |\vx - \vv| < \de\} \subset U$$Since $$S_n(\vv, \de) = \{\vx \in \R^n : |\vx - \vv| < \de\}$$ is compact and $T$ is continuous, $T(S_n(\vv, \de))$ is compact. Define $f: \R^n \to \R$ as $f(\vw) = |\vw - T\vv|$, which is the compisition of continuous functions and is thus continuous. Then $f(T(S_n))$ is compact, so it has some minima $\alpha \ge 0$. Suppose $\alpha = 0$. Then there exists some $\vz \in S_n(\vv, \de)$ such that $|T\vz - T\vv| = 0$ and thus $T(\vz - \vv) = 0$. Since $\vz \neq \vv$, $\nul T \ge 1$, so $\im T \neq \R^n$, and thus there exists some $\vw \in \R^n \setminus \im T$. Let $\vv' \in O$, there exists an open ball $B(T\vv', \e) \subset TO$. Then $$\left|\frac{\e}{2|\vw|}\vw + T\vv - T\vv\right| = \frac{\e}{2} < \e$$so $\frac{\e}{2|\vw|}\vw + T\vv \in B(T\vv', \e)$ and thus $\frac{\e}{2|\vw|}\vw + T\vv \in B(T\vv', \e) \in TO$. Then $\frac{\e}{2|\vw|}\vw + T\vv = T\vu$ for some $\vu \in O$, so $\vw = T(\frac{2|\vw|)}{\e}(\vu - \vv))$, so $\vw \in \im T$, which is a contradiction. Hence, $\nul T = 0$, so $\rank T = n$ and thus $\im T = \R^n$.
            
            From there we see that $\alpha > 0$. Let $\vw \in B(T\vv, \alpha)$. As we proved in the previous paragraph, $\im T = \R^n$, so there exists some $\vu \in \R^n$ such that $T\vw = \vu$. Suppose $|\vu - \vv| \ge \delta$. Define $f: [0,1] \to \R$ such that $f(t) = |t\vu + (1-t)\vv - t\vv|$. $f$ is continuous, $f(0) = 0$, and $f(1) = |\vu - \vv| \ge \delta$, so by the intermediate value theorem, there exists some $t \in (0,1]$ such that $f(t) = \delta$. Then $|t\vu + (1-t)\vv - \vv| = \delta$. Then $t\vu + (1-t)\vv \in S_n(\vv, \de)$, so $T(t\vu + (1-t)\vv) \in TS_n(\vv, \de)$, so $|T(t\vu + (1-t)\vv) - T\vv| \ge \alpha$. Since $|T(t\vu + (1-t)\vv) - T\vv| = t|T\vu - T\vv|$, we have $t|T\vu - T\vv| \ge \alpha$. $t \in (0,1]$ implies that $|T\vu - T\vv| \le \frac{\alpha}{t} \le \alpha$ which implies $T\vu = \vw \notin B(T\vv, \alpha)$, which is a contradiction. Therefore, if $\vw \in B(T\vv, \alpha)$, then $\vw = T\vu$ for some $\vu \in B(\vv, \delta)$. $B(\vv, \delta) \subset U$ implies $\vu \in U$ which implies $\vw \in TU$. Thus, $B(T\vv, \alpha) \subset TU$. Since there exists such a region around every $\vx \in TU$, $TU$ is open.
            \item Suppose there exists a nonempty open set $O$ such that $TO$ is open. Following from that, in the first paragraph of part $b$, we proved that $\nul T = 0$, so $T$ is injective, and $\rank T = n$, so $T$ is surjetive, and thus $T$ is bijective.
            
            Suppose $T$ is bijective. Then $T^{-1}$ is a bijective linear transformation. By a, it is continuous. Then $T$ is the inverse of a continuous function, so for any open $U \in \R^n$, $TU$ is open.
        \end{enumerate}
    \end{solution}

    \begin{customex}{\textbf{1}}
        Two finite dimensional vector spaces $V, W$ with a corresponding inner product $\cdot$ and $\tilde{\cdot}$ are isomorphic if there is a bijective linear map $T : V \to W$ so that $T(x)\tilde{\cdot}T(y) = x\cdot y$ for all $x, y \in V$. The map $T$ is called an isometry.\\
        Show that any two finite dimensional vector spaces $V, W$ with a corresponding inner product $\cdot$ and $\tilde{\cdot}$ are isomorphic if and only if $\dim V= \dim W$.
    \end{customex}

    \begin{solution}
        Suppose $V, W$ are isomorphic. Then there exists a bijection $T:V \to W$ between them. Let $\{\vv_1, \cdots, \vv_k\}$ be a basis of $V$. Since $T$ is injective, $T\vv_1, \cdots, T\vv_k$ is linearly independent in $W$. Fix $\vw \in W$. Since $T$ is surjective, there exists a $$\vv = \sum_{i = 1}^n \alpha_i \vv_i$$such that $$T\vv = T\sum_{i = 1}^n \alpha_i \vv_i = \sum_{i = 1}^n \alpha_i T\vv_i$$so $\vw \in \spn \{T\vv_1, \cdots, T\vv_k\}$. Since $T\vv_1, \cdots, T\vv_k$ is linearly independent and spanning, it is a basis. Therefore, $\dim V = \dim W$.

        Suppose $\dim V = \dim W$. Let $\vv_1, \cdots, \vv_n$ be an orthonormal basis of $V$ and $\vw_1, \cdots, \vw_n$ be an orthonormal basis of $W$. Define $T:V \to W$ such that $T\vv_i = \vw_i$. Fix $\vv, \vw \in V$. Then $$\vv = \sum_{i = 1}^n \alpha_i \vv_i$$and $$\vw = \sum_{i = 1}^n \beta_i \vw_i$$Then since $\vw_1, \cdots, \vw_n$ is an orthonormal basis we have
        \begin{align*}
            T\vv \cdot T\vw &= T\left(\sum_{i = 1}^n \alpha_i \vv_i\right) \cdot T\left(\sum_{i = 1}^n \beta_i \vv_i\right)\\
            &= \left(\sum_{i = 1}^n \alpha_i T\vv_i\right) \cdot T\left(\sum_{i = 1}^n \beta_i T\vv_i\right)\\
            &= \left(\sum_{i = 1}^n \alpha_i \vw_i\right) \cdot \left(\sum_{i = 1}^n \beta_i \vw_i\right)\\
            &= \sum_{i = 1}^n \alpha_i\beta_i\vw^2\\
            &= \sum_{i = 1}^n \alpha_i\beta_i
        \end{align*}

        Similarly, since $\vv_1, \cdots, \vv_n$ is an orthonormal basis of $V$
        \begin{align*}
            \vv \cdot \vw &= \left(\sum_{i = 1}^n \alpha_i \vv_i\right) \cdot \left(\sum_{i = 1}^n \beta_i \vv_i\right)\\
            &= \sum_{i = 1}^n \alpha_i\beta_i\vw^2\\
            &= \sum_{i = 1}^n \alpha_i\beta_i
        \end{align*}

        Thus $T\vv \cdot T\vw = \vv \cdot \vw$, so an isometry exists between $V$ and $W$.
    \end{solution}

    \begin{customex}{\textbf{2}}
        $V$ and $W$ are two finite dimensional vector spaces with corresponding inner products. (On the assignment, d was skipped. I did not skip it in my solution, so the following items are offset by one).
        
        \begin{enumerate}[label = \alph*)]
            \item Show that the adjoint map $T^*$ of $T: V \to W$ is unique.
            \item Show that the adjoint of $T^*$ is $T$.
            \item Show that $\rank(T^*) = \rank(T)$.
            \item Show that $\im(T) = \ker(T^*)^{\perp}$
            \item Show that $\im(T^*) = \ker(T)^{\perp}$
            \item Using c) and d) show that $\dim(V) = \nul(T) - \rank(T)$
            \item Show that $\dim(V) - \dim(W) = \nul(T) - \nul(T^*)$
        \end{enumerate}
    \end{customex}

    \begin{solution}
        \begin{enumerate}[label = \alph*)]
            \item Suppose there exists $S, S':W \to V$ such that $\vw \cdot T\vv = \vv \cdot S\vw = \vv \cdot S'\vw$. Let $\{\vw_1, \cdots, \vw_n\}$ is a basis of $W$ and $\vv \neq \vzero \in V$ be arbitarary. Then for any $1 \le i \le n$, $\vw_i \cdot T\vv = \vv \cdot S\vw_i = \vv \cdot S'\vw_i$, so $\vv \cdot (S\vw_i -S'\vw_i) = 0$. Since $\vv$ is arbitrary, set $\vv = (S\vw_i -S'\vw_i)$ then $(S\vw_i -S'\vw_i)^2 = 0$, so $S\vw_i -S'\vw_i = \vzero$ and thus $S\vw = S'\vw$. Since $S$ equals $S'$ over a basis of $W$, $S = S'$. Thus the adjoint map of $T^*$ is unique.
            \item By definition for any $\vv \in V, \vw \in W$, $$\vv \cdot T\vw = \vw \cdot T^* \vv = \vv \cdot T^{**}\vw$$Then by the previous proof, $T = T^**$.
            \item Let $\vy = T(\vx)$ for some $\vx \in V$. Choose $\vw \in W$ such that $T^*\vw = \vzero$. Then $$\vw \cdot \vy = \vw \cdot T\vx = \vx \cdot T^* \vw = \vx \cdot \vzero = 0$$Then for all $\vw \in \ker T^*$, $\vw \cdot \vy = 0$ so $y \in (\ker T^*)^{\perp}$Therefore $\im(T) \subset \ker(T^*)^{\perp}$. Similarly $\im(T^*) \subset \ker(T)^{\perp}$. It follows that $\rank T \le \nul (T^*)^{\perp}$ and $\rank T^* \le \nul (T)^{\perp}$. Since $\rank T + \nul T = \dim V$ and $$\dim(\ker (T^*)^{\perp}) + \dim(\ker T^*) = \dim(\ker (T^*)^{\perp}) + \nul T^* = \dim W = \rank T^* + \nul T^*$$we have $$\rank T + \nul T^* \le \dim(\ker (T^*)^{\perp}) + \nul T^* = \dim W = \rank T^* + \nul T^*$$and therfore we know $\rank T^* \le \rank T$. Since $\rank T^* + \nul T^* = \dim W$ and $$\dim(\ker (T)^{\perp}) + \dim(\ker T) = \dim(\ker (T)^{\perp}) + \nul T = \dim V = \rank T + \nul T$$we have $$\rank T^* + \nul T \le \dim(\ker (T)^{\perp}) + \nul T = \dim V = \rank T + \nul T$$and therfore we know $\rank T \le \rank T^*$. Since both $\rank T \le \rank T^*$ and $\rank T^* \le \rank T$ and thus $\rank T = \rank T*$.
            \item By the previous proof, $\im(T) \subset \ker(T^*)^{\perp}$ and $\rank(T) = \rank(T^*)$. $$\rank(T^*) + \nul(T^*) = \dim W = \nul(T^*) + \dim \ker(T^*)^{\perp}$$so $\rank(T^*) = \dim \ker(T^*)^{\perp}$ and thus $\rank(T) = \dim \im T = \dim \ker(T^*)^{\perp}$. It follows that $\im T = \ker(T^*)^{\perp}$.
            \item By part c, $\im(T^*) \subset \ker(T)^{\perp}$ and $\rank(T) = \rank(T^*)$. $$\rank(T) + \nul(T) = \dim V = \nul(T) + \dim \ker(T)^{\perp}$$so $\rank(T) = \dim \ker(T)^{\perp}$ and thus $\rank(T^*) = \dim \im T^* = \dim \ker(T)^{\perp}$. It follows that $\im T^* = \ker(T)^{\perp}$.
            \item By part c and d,  
            \begin{align}
                \rank(T) &= \rank(T^*)\\
                &= \vdim \im(T^{*}) \\
                &= \vdim \ker(T)^{\perp} \\
                &= \dim V - \dim \ker(T)\\
                &= \dim V - \nul (T)
            \end{align}
    
            Line $4$ follows from $\dim W + \dim W^{\perp} = \dim V$, and the others from c and d. Hence, we have $\rank (T) = \dim V - \nul (t)$, so $\rank (t) + \nul (t) = \dim V$.
            \item $\nul T + \rank T = \dim V$ and $\nul T^* + \rank T^* = \dim W$. By c, $\rank T = \rank T^*$, so $\nul T^* + \rank T = \dim W$. Then $$\dim V - \dim W = \nul T + \rank T - (\nul T^* + \rank T) = \nul T - \nul T^*$$
        \end{enumerate}
    \end{solution}

    \begin{customex}{\textbf{3}}
        $V$ is a finite dimensional vector space with a corresponding inner product.
        \begin{enumerate}[label=\alph*)]
            \item Show that if $T : V \to V$ is a linear map so that $|T(v)| = |v|$ for all $v \in V$ then $T$ is an isometry.
            \item Show that if $\{T_i\}_{i\in \N}$ is a sequence of isometries of $V$ , there is an isometry $T$ of $V$ so that, after passing
            to a subsequence, $$\lim_{t \to \inf} T_i(v) = T(v)$$
        \end{enumerate}
    \end{customex}

    \begin{solution}
        \begin{enumerate}[label = \alph*)]
            \item Suppose $T$ is a linear map such that $|T(\vv)| = |\vv|$ for all $\vv \in V$. Fix $\vv, \vw \in V$. Then $|T\vv + T\vw|^2 = |\vv + \vw|^2$. By the distributive property, $|T\vv + T\vw|^2 = T\vv^2 + 2T\vv T\vw + T\vw^2$ and $|\vv + \vw|^2 = \vv^2 + 2\vv\vw = \vw^2$. Since $|T\vv| = |\vv|, T\vv^2 = \vv^2$ and similarly $T\vw^2 = \vw^2$, so $T\vv^2 + 2T\vv T\vw + T\vw^2 = |T\vv + T\vw|^2 = |\vv + \vw|^2 = \vv^2 + 2\vv\vw = \vw^2 = T\vv^2 + 2\vv\vw + T\vw^2$ and thus $T\vv^2 + 2T\vv T\vw + T\vw^2 = T\vv^2 + 2\vv \vw + T\vw^2$. Subtracting $T\vv^2 + T\vw^2$ and dividng by $2$ leaves $T\vv T\vw = \vv\vw$, which by definition means $T$ is an isometry.
            \item First we will prove that for arbitrary $\vv \in V$, a subsequence of $T_i(\vv)$ converges to some point $\vw$. Consider the set $S = \{T_i\vv\}_{i \in \N}$. If $S$ is finite, then an infinite number of $T_i(\vv) = \vw$, so the subsequence converges to $\vw$. Otherwise, suppose $S$ has no limit points. Then it is closed, and since $S$ is bounded by $|\vv|$, $S$ is compact. Since $S$ has no limit points, every point $\vp \in S$ has some ball $B(\vp, \de_\vp)$, $\de_\vp > 0$ containing $\vp$ such that $B(\vp, \de) \cap S = \{\vp\}$. The set $$G = \{B(\vp, \de_\vp)\}_{\vp \in S}$$is an open cover of $S$, so since $S$ is compact, $G$ has a finite subset $G'$ with a finite number of balls. Since each ball contains a single element of $S$, $G'$ has a finite number of elements of $S$. However, $S \subset G'$, so $G'$ has a infinite number of elements of $S$, which is a contradiction. Thus if $S$ is infinite then it has some limit point $\vp$. Then for any $\de > 0$, $B(\vp, \de) \cap S$ has an infinite number of elements of $\{T_i(\vv)\}$, so we can define a subsequence which converges to $\vp$. 
            
            We will define $T:V \to W$ as $T\vv = \lim_{T_i\vv}$. We have already proven $T$ is well defined (i.e. $\lim_{T_i\vv}$ converges for all $\vv$), now we will prove it is a linear transformation. Let $\vv, \vw \in V$ and $\alpha \in \R$. 

            \begin{align*}
                \alpha T\vv &= \alpha \lim_{i \to \inf} T_i\vv \\
                &= \lim_{i \to \inf} \alpha T_i\vv\\
                &= \lim_{i \to \inf} T_i\alpha \vv\\
                &= T\alpha \vv\\
                \text{and}\\
                T\vv + T\vu &= \lim_{i \to \inf} T_i\vv + \lim_{i \to \inf} T_i\vw\\
                &= \lim_{i \to \inf} T_i \vv + T_i \vw\\
                &= \lim_{i \to \inf} T_i (\vv + \vw)\\
                &= T(\vv + \vw)
            \end{align*}

            Thus $T$ is linear. Since $|\cdot|$ is continuous and $\lim_{i \to \inf} T_i\vv = T\vv$, $$|\vv| = \lim_{i \to \inf} |\vv| = \lim_{i \to \inf} |T_i\vv| = |T\vv|$$Then by part a, since $|\vv| = |T\vv|$ and $T$ is a linear map, $T$ is an isometry.
        \end{enumerate}
    \end{solution}

    \begin{customex}{\textbf{4}}
        $V$ a finite dimensional vector space with dimension $2$ or higher.
        \begin{enumerate}[label = \alph*)]
            \item Find $T, S$ linear maps from $V$ to $V$ so that $S \circ T \neq T \circ S$.
            \item Show that $T^2 = T$ if and only if $T = \frac{I + B}{2}$ or $T = \frac{I - B}{2}$ where $B^2 = I$ and $T^2 = T \circ T$.
        \end{enumerate}
    \end{customex}

    \begin{solution}
        \begin{enumerate}[label = \alph*)]
            \item Let $\{\vv_1, \cdots, \vv_n\}$ be a basis of $V$ and $$T(\sum_{i=1}^n \alpha_i \vv_i) = \sum_{i=2}^n \alpha_{i-1}\vv_i$$and$$S(\sum_{i=1}^n \alpha_i \vv_i) = \alpha_n \vv_n$$Then $T\circ S(\sum_{i=1}^n \alpha_i \vv_i) = T(\alpha_n\vv_n) = \vzero$ but $S \circ T(\sum_{i=1}^n \alpha_i \vv_i) = S(\sum_{i=2}^n \alpha_{i-1}\vv_i) = \alpha_{n-1}\vv_n$. For $\alpha_{n-1} \neq 0$, these are unequal, so $T \neq S$.
            \item Suppose $T = \frac{I + B}{2}$ and $B^2 = I$. Then $$T^2 = \left(\frac{I + B}{2}\right)\left(\frac{I + B}{2}\right) = \frac{I^2 + IB + BI + B^2}{4} = \frac{I^2 + B + B + I}{4} = \frac{2I + 2B}{4} = \frac{I + B}{2} = T$$Therefore, $T^2 = T$.\\Similarly, suppose $T = \frac{I - B}{2}$. Then $$T^2 = \left(\frac{I - B}{2}\right)\left(\frac{I - B}{2}\right) = \frac{I^2 -IB -BI + B^2}{4} = \frac{I^2 - B - B + I}{4} = \frac{2I - 2B}{4} = \frac{I - B}{2} = T$$Therefore $T^2 = T$. 
            
            Suppose $T^2 = T$. Let $B = 2T - I$. Then $B^2 = 4T^2 - 4T + I = 4T - 4T + I = I$, so if $T = \frac{I + B}{2}$ then $B^2 = I$. Let $B = I - 2T$ then $B^2 = I - 4T + 4T^2 = I - 4T + 4T = I$. Then $B^2 = I$, so if $T + \frac{I - B}{2}$ then $B^2 = I$.
        \end{enumerate}
    \end{solution}

    \begin{customex}{\textbf{5}}
        \begin{enumerate}[label = \alph*)]
            \item Show that $|\vv\cdot \vw| \le |\vv||\vw|$ (Cauchy-Schwartz inequality) and equality happens if and only if $\vv$ and $\vw$ are multiples of each other.
            \item Let $W$ be a subspace of $V$. Show that any vector $\vv$ in $V$ can be uniquely expressed as $\vv = \vw + \vw^{\perp}$ where $\vw \in W$ and $\vw^{\perp} \in W^{\perp}$
            \item Using the notation above consider the orthogonal projection $P : V \to V$ so that $P(\vv) = \vw$. Show that
            $P$ is linear and that $P(\vv)$ is the closest point in $W$ to $\vv$, i.e.,
            $|P(\vv) - \vv| \le |\vw - \vv|$ for all $\vw \in W$.
        \end{enumerate}
    \end{customex}

    \begin{solution}
        \begin{enumerate}[label = \alph*)]
            \item First we will cover the general case. Let $$\vx = \vw - \frac{\vv \cdot \vw}{\vv^2} \vv$$Then $$\vv \cdot \vx = \vv \cdot \vw - \frac{\vv \cdot \vw}{\vv^2} \vv^2 = \vv \cdot \vw - \vv \cdot \vw =  0$$so $\vv \perp \vx$. Since $\vx + \frac{\vv \cdot \vw}{\vv^2} \vv = \vw$, we can use the pythagorean theorem on a triangle with legs $\vx,  \frac{\vv \cdot \vw}{\vv^2} \vv$ and hypotenuse $\vw$ to see $$\vx^2 + \frac{(\vv \cdot \vw)^2}{\vv^4} \vv^2 = \vx^2 + \frac{(\vv \cdot \vw)^2}{\vv^2}= \vw^2$$Since $\vx^2 > 0$, we have $\frac{(\vv \cdot \vw)^2}{\vv^2} \ge \vw^2$ which implies $(\vv \cdot \vw)^2 \ge \vw^2 \vv^2$ and by taking the square root $|\vv \cdot \vw| \ge |\vw| |\vv|$. In the case that $\vw = \alpha \vv$, $\alpha \in \R$, then in the solution above, $$\vx = \vw - \frac{\vv \cdot \vw}{\vv^2} \vv = \vw - \alpha \frac{\vv^2}{\vv^2} \vv = \vw - \vw = \vzero$$The remainder of the solution remains the same, except we are left at the end with $\frac{(\vv \cdot \vw)^2}{\vv^2}= \vw^2$ and thus $(\vv \cdot \vw)^2 = \vv^2 \vw^2$ finally leaving $|\vv \cdot \vw| = |\vv||\vw|$.
            
            \item Let $\{\vw_1, \dots, \vw_n\}$ be a basis of $W$ and $\{\vu_1, \dots, \vu_k\}$ be a basis of $W^{\perp}$. Then $\dim V = n + k$. Suppose for some $\vu_i = \sum_{i = 1}^n \alpha_i \vw_i$. For some $1 \le i \le n $ $\alpha_i \neq 0$, so $\vu_i \cdot \vw_i = \alpha_i \neq 0$, and thus $\vu_i \not \in W^{\perp}$, which is a contradiction. Then all $\vu_i$ is linear independent to $\{\vw_1, \dots, \vw_n\}$. By similar reasoning, all $\vw_i$ is linear independent to $\{\vu_1, \dots, \vu_k\}$. Thus $\{\vw_1, \dots, \vw_n,\vu_1, \dots, \vu_k \}$ is a linearly independent list of length $n + k$ in $V$, so it is a basis of $\dim V$. Fix $\vv \in V$. Then $$\vv = \sum_{i = 1}^n \alpha_i \vw_i + \sum_{i = 1}^k \beta_i \vu_i$$Let $$w = \sum_{i = 1}^n \alpha_i \vw_i$$so $w \in W$ and$$w^{\perp} = \sum_{i = 1}^k \beta_i \vu_i$$so $w^{\perp} \in W^{\perp}$ and finally $\vv = \vw + \vw^{\perp}$.
            
            \item First we will prove $P$ is a linear map. Fix $\vv, \vu \in V$ such that $\vv = \vw + \vw^{\perp}$ and $\vu = \vx + \vx^{\perp}$, $\vw, \vx \in W$, $\vw^{\perp}, \vx^{\perp} \in W^{\perp}$. Since $\vv = \vw + \vw^{\perp}$, $\alpha\vv = \alpha\vw + \alpha\vw^{\perp}$. $W, W^{\perp}$ are subspace,s so $\alpha\vw + \vx \in W$ and $\alpha\vw^{\perp} + \vx^{\perp} \in W^{\perp}$. Thus $\alpha\vv + \vu = (\alpha\vw + \vx) + (\alpha\vw^{\perp} + \vx^{\perp})$, so $P(\alpha\vv + \vu) = \alpha\vw + \vx = \alpha P(\vv) + P(\vu)$. Thus $P$ is linear. 
            
            Now we will prove $|P(\vv) - \vv| \le |\vx - \vv|$ for all $\vx \in W$. Let $\vx \in W$ and set $\vv = \vw + \vw^{\perp}$. Define $\vu = \vw - \vx$. Since $\vw \in W$ and $\vx \in W$, $\vu \in W$. Then $\vv -\vx = \vw^{\perp} + \vw - \vw + \vu = \vw^{\perp} + \vu$. $$(\vu + \vw^{\perp})^2 = \vu^2 +2\vu\vw^{\perp} + \vw^2$$Since $\vu \in W$, $\vu\vw^{\perp} = 0$, so $(\vu + \vw^{\perp})^2 = \vu^2 + \vw^2 \ge \vw^2$. Therefore $|\vu + \vw^{\perp}| \ge |\vw^{\perp}|$. As we previously showed, $\vv - \vx = \vw^{\perp} + \vu$ and since $\vv = \vw + \vw^{\perp}$ and $P(\vv) = \vw$, $P(\vv) - \vv = \vw^{\perp}$. Combining these with the previous inequality gives $|P(\vv) - \vv| \le |\vv - \vx|$ for all $\vx \in W$.
        \end{enumerate}
    \end{solution}

    \begin{customex}{\textbf{6}}
        $V$ a finite dimensional vector space with an inner product and $V^*$ the set of all linear maps from $V$ to $\R$, which is itself a finite dimensional vector space.
        \begin{enumerate}[label = \alph*)]
            \item (Riesz Representation Theorem) Given $f \in V^*$, show there is a unique $\vv \in V$ so that $f(\vx) = \vx\cdot \vv$ for all $\vx \in V$. Denote such $\vv$ by $\vv_f$.
            \item Show that the map $Q_V : V^* \to V$, $Q_V(f) = \vv_f$ is linear and bijective.
            \item Consider $W$ another finite dimensional vector space with another inner product. Given a linear map $T : V \to W$, show that the map below is linear
            
                $$\mathbf{T^*}:W^* \to V^*, T*(f)(\vv) = f(T(\vv)), \vv \in V$$
            \item Show that $T^* = Q_V \circ \mathbf{T^*} Q_W^{-1}$ where $Q_W^{-1}$ is the inverse of $Q_W$ and $T^*$ is the adjoint of $T$.
        \end{enumerate}
    \end{customex}

    \begin{solution}
        \begin{enumerate}[label = \alph*)]
            \item Let $\dim V = n$ Since $\dim \R = 1$, $\rank V \le 1$ so by Rank + Nullity either $\nul f = n$, in which case $f(\vv) = 0$ for all $\vv$, so $\vv_f = \vzero$, or $\nul f = n-1$. In that case, let $\{\vv_1, \dots, \vv_{n-1}\}$ be an orthonormal basis of $\ker f$ and let $\vw$ extend it to an orthonormal basis of $V$. Let $\vv_f = f(\vw)\vw$. Fix $\vv \in V$, so $\vv = \vx + \alpha \vw$, where $\vx \in \ker f$, $\alpha \in \R$. Then $f(\vv) = f(\vx) + \alpha f(\vw) = \alpha f(\vw)$. Additionally, since $\vw$ is normal to the basis of $\ker f$, $\vw$ is normal to every element in $\ker f$ and thus $\vw \cdot \vx = 0$. Since $|\vw| = 1$, $\vw \cdot \vw = 1$. Using that we see the following:
                \begin{align*}
                    \vv \cdot \vv_f &= \vv \cdot f(\vw)\vw\\
                    &= f(\vw)((\vx + \alpha\vw)\cdot \vw)\\
                    &= f(\vw)(\vx \cdot \vw + \alpha\vw\cdot\vw)\\
                    &= f(\vw)(0 + \alpha)\\
                    &= \alpha f(\vw)
                \end{align*}
            Thus, $f(\vv) = \alpha f(\vw) = \vv \cdot \vv_f$. Therefore there exists such a $\vv_f$ for every $f$. Suppose there exists $\vv_f'$ such that $f(\vv) = \vv \cdot \vv_f'$. Then $f(\vv_f') = \vv_f' \cdot \vv_f = \vv_f' \cdot \vv_f'$ and $f(\vv_f) = \vv_f \cdot \vv_f = \vv_f \cdot \vv_f'$. Then we have $\vv_f^2 = \vv_f\cdot \vv_f' = \vv_f'^2$, so $(\vv_f\vv_f')^2 = \vv_f'^2\vv_f^2$, and finally $|\vv_f\vv_f'| = |\vv_f||\vv_f'|$. Then by 5.a, $\vv_f' = \beta \vv_f$, $\beta \in \R$. We already showed that $f(\vv_f') = \vv_f\cdot \vv_f' = f(\vv_f)$, so $f(\vv_f') = f(\beta \vv_f) = \beta f(\vv_f)$ and thus $f(\vv_f) = \beta f(\vv_f)$, so $\beta = 1$ and therefore $\vv_f' = \vv_f$. From this we see that there exists a unique $\vv_f \in V$ such that for all $\vv \in V$, $f(\vv) = \vv \cdot \vv_f$.
            \item First to prove bijectivity. Fix $\vv \in V$ and define $f:V \to \R$ such that $f(\vx) = \vx \cdot \vv$. Then $Q_V(f) = \vv$. Therefore $Q_V$ is surjective. Fix $f, g \in V^*$ and suppose $Q_v(f) = Q_v(g) = \vv$. Then for all $\vx \in V$, $f(\vx) = \vv \cdot \vx = g(\vx)$, so $f = g$ and thus $Q_V$ is injective and therefore bijective. 
            
            Now for linearity. Let $\alpha \in \R$ and $f, g \in V^*$. Then $(\alpha f)(\vv) = \alpha (\vv_f \cdot \vv) = (\alpha \vv_f) \cdot \vv$, so $\vv_{\alpha f} = \alpha \vv_f$ and thus $Q_V(\alpha f) = \alpha Q_V(f)$. Additionally, $f(\vv) + g(\vv) = \vv_f \cdot \vx + \vv_g \cdot \vx = (\vv_f + \vv_g) \cdot \vx$, so $\vv_{f+g} = \vv_f + \vv_g$, and thus $Q_V(f) + Q_V(G) = Q_V(f+g)$. It follows that $Q_V$ is linear.
            
            \item Fix $\alpha \in \R$, $f, g \in W^*$. Then for ant $\vv \in W$, $T^*(\alpha f + g)(\vv) = (\alpha f + g)(T(\vv)) = \alpha f(T(\vv)) + g(T(\vv)) = (\alpha T^* f + T^* g)(\vv)$. Thus $T^*(\alpha f + g)(\vv) = (\alpha T^* f + T^* g)(\vv)$ so $T*$ is linear.
            \item Fix $\vw \in W$. $Q^{-1}_W(\vw) = f$ where for all $\vx \in W$, $f(\vx) = \vw \cdot \vx$. $Q_V(\mathbf{T^*(f)}) = \vv_{f\circ T}$, so $Q_V \circ \mathbf{T^*} Q_W^{-1}(\vw) = \vv_{f\circ T}$. For any $\vv \in V$, since $f(\vx) = \vw \cdot \vx, f(T\vv) = \vw \cdot T\vv = \vv \cdot T^*\vw$, so $f\circ T(\vv) = \vv \cdot T^*\vw$ and thus $\vv_{f\circ T} = T^*\vw$. Therefore, $T^*\vw = Q_V \circ \mathbf{T^*} Q_W^{-1}(\vw)$ so $T^* = Q_V \circ \mathbf{T^*} Q_W^{-1}$.
        \end{enumerate}
    \end{solution}

    \begin{customex}{\textbf{7}}
        Set $\poly_n = \{\sum_{i=0}^n a_it^i : a_i \in \R, i = 1, \dots, n\}$ the set of all polynomials with degree $\le n$.Consider the dot product $$p \cdot q = \int_0^1 p(t)q(t) dt$$
        
        \begin{enumerate}[label = \alph*)]
            \item Show that $\cdot$ indeed is an inner product.
            \item Find an orthonormal basis for $\poly_4$.
            \item Consider the linear maps $T,R: \poly_n \to \poly_n$.
                $$T(p)(t) =\int_0^1 (t-x)p(x) dx \text{ and } R(p)(t) = \int_0^1 (t-x^2)p(x) dx$$
            Show that $T^* = -T$ and compute $R^*$ (for this last case maybe you want to split cases $n \ge 2$ and $n = 1$)
            \item Use rank + nullity to conclude that given $q \in \poly_n$, $T(p) = q$ has a solution if and only if $q \in \spn\{1,t\}$.
        \end{enumerate}
    \end{customex}

    \begin{solution}
        \begin{enumerate}[label = \alph*)]
            \item Let $p,q,r \in \poly_n$ and $\alpha \in \R$. Then 
            
            \begin{align*}
                (\alpha p + q) \cdot r &= \int_0^1 (\alpha p + q)(t)r(t)dt \\
                &= \int_0^1 \alpha p(t)r(t) dt + \int_0^1 q(t)r(t) dt \\
                &= \alpha \int_0^1 p(t)r(t) dt + \int_0^1 q(t)r(t) dt\\
                &= \alpha (p \cdot r) + (q \cdot r)
            \end{align*}

            Since multiplication of polynomials is commutative, $p \cdot q = q \cdot p$. For any $p \in \poly_n$ if $p \neq 0$ then $p^2(t) \ge 0$ for all $t \in [0,1]$ and $p(t) > 0$ for some region in $[0,1]$, so $\int_0^1 p^2(t) dt > 0$, and thus $p\cdot p > 0$. Suppose $p = 0$. Then $p(t) = 0$ for all $t \in [0,1]$, so $\int_0^1 p(t)dt = 0$, and thus $p \cdot p = 0$. Since all of these properties hold, $\cdot$ is an inner product.

            \item Using the Gram-Schmidt process, an orthonormal basis of $\poly_4$ is 
            
            We will use the basis $\{1, t, t^2, t^3, t^4\}$ as our starting basis $\{\vv_1, \dots, \vv_n\}$, use it to find an orthogonal basis $\{\vu_1, \dots, \vu_n\}$, and then convert it to an orthonormal basis $\{e_1, \dots, e_n\}$

            $\vu_1 = \vv_1 = t$, so $\e_1 = \frac{1}{|1|} = 1$.\\$\vu_2 = \vv_2 - \frac{\vv_2 \cdot \vu_1}{|\vu_1|} \vu_1 = t - \frac{1}{2}$. $|\vu_2| = \frac{\sqrt{3}}{6}$, so $e_2 = 2\sqrt{3}x - \sqrt{3}$.\\$\vu_3 = \vv_3 - \frac{\vv_3\cdot \vu_1}{|\vu_1|}\vu_1 - \frac{\vv_3 \cdot \vu_2}{|\vu_2|}\vu_2 = t^2-t + \frac{1}{6}$. $|\vu_3| = \frac{1}{6\sqrt{5}}$, so $e_3 = 6\sqrt{5}t^2 - 6\sqrt{5}t + \sqrt{5}$.\\Continuing this process for the final 2 elements, we get orthonormal basis $\{e_1, \dots, e_n\} = $ 
            $$\{1, 2\sqrt{3}t - \sqrt{3}, 6\sqrt{5}t^2 - 6\sqrt{5}t + \sqrt{5}, 20\sqrt{7}t^3 - 30\sqrt{7}t^2 + 12\sqrt{7}t - \sqrt{7},210t^4-420t^3+270t^2-60t + 3\}$$
            \item Following from the definition of $T$, $Tp = p(x) \cdot (t-x) = t(p(x)\cdot 1) - (p(x) \cdot x)$, so 
            \begin{align*}
                q(t) \cdot Tp &= q(t) \cdot (t(p(x)\cdot 1) - (p(x) \cdot x))\\
                &= (q(t) \cdot t)(p(x) \cdot 1) - (p(x) \cdot x)(q(t) \cdot 1)\\
                &= -((p(x) \cdot x)(q(t) \cdot 1) - (q(t) \cdot t)(p(x) \cdot 1))\\
                &= -(p(x) \cdot (x(q(t) \cdot 1) - (q(t) \cdot t)))\\
                &= -p \cdot Tq
            \end{align*}

            For $R$, we will consider two cases: when $n \ge 2$ and when $n = 1$ ($n = 0$ is impossible because the image of $R$ is a polynomial with degree one and thus is not in $\poly_0$). If $n \ge 2$, let $R^*(q)(x) = \int_0^1 (t-x^2)q(t)dt$. Then for $p, q \in \poly_n$, 
            \begin{align*}
                p \cdot R^*q &= \int_0^1 p(x) \int_0^1 (t-x^2)q(t)dt \\
                &= \int_0^1 \int_0^1 p(x)(t-x^2)q(t)dt dx\\
                &= \int_0^1 q(t) \int_0^1 p(x)(t-x^2)dxdt\\
                &= q \cdot Rp
            \end{align*}
            Thus $R^*$ is the adjoint. 

            For the $n = 1$ case, define $R^*$ over $p_0(t) = 1$ and $p_1(t)=t$, which are a basis of $\poly_1$, as the following: $R^*(p_0) = \frac{2}{3}-t$ and $R^*(p_1) = \frac{5}{12} - \frac{t}{2}$. Fix $p, q \in \poly_1$ where $p = \alpha + \beta t$ and $q = \mu + \nu t$. Then 
            
            \begin{align*}
                p \cdot Rq &= (\alpha + \beta t) \cdot R (\mu + \nu t)\\
                &= \int_0^1 (\alpha + \beta t) \int_0^1 (t - x^2) (\mu + \nu x) dx dt\\
                &= \frac{4 \alpha \mu + \beta (r \mu + \nu)}{24}
            \end{align*}
            and since $$R^*(\alpha + \beta t) = \alpha R^*(p_0) + \beta R^*(p_t) = \alpha (\frac{2}{3}-t) + \beta(\frac{5}{12} - \frac{t}{2}) = \frac{5\beta + 8 \alpha}{12} - \frac{t(\alpha+2\beta)}{2}$$we have
            \begin{align*}
                q \cdot Rq &= (\mu + \nu t) \cdot R^* (\alpha + \beta t)\\
                &= \int_0^1 (\mu + \nu t) \left(\frac{5\beta + 8 \alpha}{12} - \frac{t(\alpha+2\beta)}{2}\right)dt\\
                &= \frac{4 \alpha \mu + \beta (r \mu + \nu)}{24}
            \end{align*}

            Thus $p Rq = q R^* p$, as required.
            \item We will first prove that $\ker (T^*)^{\perp} = \spn\{1, t\}$. Suppose $p \in \spn\{1, t\}$. Then $p = \alpha + \beta t$ for $\alpha, \beta \in \R$. Fix $q \in \ker T^*$. We know $T^* q = 0$, so $(\alpha + \beta t) \cdot T^*(q) = 0$, and thus $\alpha \cdot  T^*(q) = -\beta t \cdot T^*(q)$. It follows that $\int_a^b\alpha (t-x) q(t)dt dx = \int_a^b -\beta (t-x) q(t)dt dx$, so therefore $\alpha q(t) = -\beta t \cdot q(t)$, and thus $(\alpha + \beta t) \cdot q(t) = 0$, so since this holds for all $q(t) \in ker T^*$, we have $\alpha + \beta t = p(t) \in \ker (T^*)^{\perp}$. Thus $\spn\{1, t\} \subset \ker (T^*)^{\perp}$, so by rank + nullity, $\spn\{1, t\} \subset \im T$. Suppose $q \in \im T$. Then $q = (t-x) \cdot p(x) = t (1 \cdot p(x)) - x \cdot p(x)$ for some $p \in \poly_n$, so $q = \alpha + \beta t$, so $\dim \im T \le 2$. Since $\spn\{1, t\} \subset \im T$, $\rank T \ge 2$, so it follows that $\rank T = 2$. Since $\dim \spn\{1,2\} =2 $ and $\spn\{1,2\} \subset \im T$, it follows that $\im t = \spn\{1,t\}$.
        \end{enumerate}
    \end{solution}

    \begin{customex}{\textbf{8}}
        Let $T$ be a self-adjoint linear operator over $V$.
        \begin{enumerate}[label = \alph*)]
            \item $T(\vv + t\vw) \cdot (\vv + t\vw) = T\vv + 2tT\vw \cdot \vv + t^2 T\vw \cdot \vw$ for all $\vv, \vw \in V$ and $t \in \R$. 
            \item Arrange the eigenvalues $\lambda_1 \le \lambda_2 \le \dots \le \lambda_n$, where $n = \dim V$ and for each 2-dimensional subspace $P \subset V$ set $\bar T(P) = \max{T\vv \cdot \vv : \vv \in P, |\vv| = 1}$. Show that
                $$\lambda_2 = \inf{\bar T(P) : P \text{ is subspace with }\dim(P) = 2}.$$
        \end{enumerate}
    \end{customex}

    \begin{solution}
        \begin{enumerate}[label = \alph*)]
            \item Since $T$ self-adjoint, $\vv T \vw = \vw T \vv$, which will be used below
            \begin{align*}
                T(\vv + t \vw) \cdot (\vv + t \vw) &= T\vv \cdot \vv +  tT\vw \cdot \vv + tT\vv \cdot \vw + t^2 T\vw \cdot \vw\\
                &= T\vv \cdot \vv + 2tT\vw \cdot \vv + t^2 T\vw \cdot \vw
            \end{align*}
            \item Let $\{\vv_1, \dots, \vv_n\}$ be the orthonormal basis of $V$ with respect to $\lambda_1, \dots, \lambda_n$. Let $P$ be an arbitrary two dimensional subspace of $V$, so $P = \spn \{\vv_a, \vv_b\}$ with $b > a$. Let $\vv = \alpha \vv_a + \beta \vv_b \in V$ such that $|\vv| = 1$. Then $|\alpha \vv_a + \beta \vv_b| = 1$, so since $\va \cdot \vb = 0$, $\sqrt{\alpha^2 \vv_a^2 + \beta^2 \vv_b^2} = 1$. By definition $\vv_a^2 = \vv_b^2 = 1$, so $\alpha^2 + \beta^2 = 1$ and thus $\alpha^2 = (1 - \beta^2)$. Additionally, 
            \begin{align*}
                T\vv \cdot \vv &= T(\alpha \vv_a + \beta \vv_b)\cdot (\alpha \vv_a + \beta \vv_b)\\
                &= (\alpha T\vv_a + \beta T\vv_b)(\alpha \vv_a + \beta \vv_b)\\
                &= (\alpha \lambda_a\vv_a + \beta \lambda_b\vv_b)(\alpha \vv_a + \beta \vv_b)\\
                &= (\alpha^2\lambda_a\vv_a^2 + \beta^2 \lambda_b \vv_b^2)\\
                &= \alpha^2 \lambda_a \beta^2\lambda_b
            \end{align*}
            So $T\vv \cdot \vv = \alpha^2 \lambda_a \beta^2\lambda_b$. Then since $\lambda_a \le \lambda_b$, $(1-\beta^2)\lambda_a \le (1-\beta^2)\lambda_b$, so $\alpha^2 \lambda_a + \beta^2 \lambda_b \le |\lambda_b|$, so $T\vv \cdot \vv \le |\lambda_b|$, so finally $|T\vv \cdot \vv| \le \lambda_b$. $|\vb \cdot T\vb| = |\lambda_b|$, so $|\lambda_b| =  \max{T\vv \cdot \vv : \vv \in P, |\vv| = 1}$ for all subsets $P$. Since there is no eigenvalue less than $\lambda_1$, for no $P$ can $|\lambda_1| = \bar{T}(P)$. However, $\bar{T}(\spn{\vv_1, \vv_2}) = |\lambda_2|$, so therefore $\lambda_2 = \inf{\bar T(P) : P \text{ is subspace with }\dim(P) = 2}$.
        \end{enumerate}
    \end{solution}

    \begin{customex}{\textbf{9}}
        $V$ a finite dimensional vector space with an inner product and $T : V \to V$ an anti self-adjoint linear map $(T^* = -T)$.
        \begin{enumerate}[label = \alph*)]
            \item Show that if $T$ is invertible then $T$ has no nonzero vector such that $T(\vv) = \lambda \vv$ for some $\lambda \in \R$.
            \item Compute the dimension of the vector spaces (in terms of $n = \dim V$)
                $$V_+ = \{T: V \to V : T^* = T\} \text{ and } V_- = \{T: V \to V : T^* = -T\},$$
            where the maps being considered are all linear.
        \end{enumerate}
    \end{customex}

    \begin{solution}
        \begin{enumerate}[label = \alph*)]
            \item Suppose there exists some non-zero $\vv \in V$ such that $T(\vv) = \lambda \vv$ for some $\lambda \in \R$. Then $\vv \cdot T(\vv) = \lambda \vv^2$, so $\vv \cdot T^* \vv = \vv \cdot T \vv = \lambda \vv^2$. Since $T^* = -T$, $\vv \cdot T^*(\vv) = \vv \cdot -T(\vv) = -\lambda \vv^2$. Then we have $\lambda \vv^2 = -\lambda \vv^2$. Since $\vv \neq \vzero$, $\vv^2 \neq 0$, so it follows that $\lambda = -\lambda$ and thus $\lambda = 0$. Then we have $T(\vv) = 0$, so $\ker T \neq \{0\}$ and therefore $\nul T > 0$ and thus $T$ is not invertible. The contrapositive of this is the statement of part a), if $T$ is invertible the $T$ has no nonzero vector such that $T(\vv) = \lambda \vv$ for some $\lambda \in \R$.
            \item Let $\{\vv_1, \cdots, \vv_n\}$ be a basis of $B$. 
            
            First we will consider $V_+$. We will prove $\{T_i\}_{1\le i\le n}$ is a basis of $V_+$ where $$T_i\left(\sum_{i=1}^n \alpha_i \vv_i\right) = \alpha_i \vv_i$$Fix $T \in V_+$. Since $T$ is self-adjoint, it is diagonalizable. Set $\lambda_i = T\vv_i$. Fix $\vv \in V$. Then 
            \begin{align*}
                T\vv &= T\left(\sum_{j=1}^n \alpha_j \vv_j \right) \\
                &= \sum_{j=1}^n \alpha_j T\vv_j \\
                &= \sum_{j=1}^n \alpha_j \lambda_j \vv_j\\ 
                &= \sum_{j=1}^n \lambda_j T_j\left(\sum_{j=1}^n \alpha_j \vv_j\right)\\ 
                &= \sum_{j=1}^n \lambda_j T_j\vv
            \end{align*}
            Thus $\{T_i\}_{1\le i\le n}$ spans $V_+$. Suppose there exists $\alpha_1, \dots, \alpha_n \in \R$ such that $\sum_{i = 1}^n \alpha_i T_i = \vzero$. Then for $\vv = \vv_1 + \vv_2 + \dots + \vv_n$, $\sum_{i=1}^n \alpha_i T_i(\vv) = \vzero$ which implies that $\sum_{i=1}^n \alpha_i \vv_i = \vzero$. Since $\vv_1, \dots, \vv_n$ are linearly independent, $\alpha_1 = \alpha_2 = \dots = \alpha_n = 0$ and thus $\{T_i\}_{1\le i\le n}$ is linearly independent and therefore is a basis. It clearly has length $n$, so $\dim V_+ = n$.

            Now we will consider $V_-$. Consider the set $$V' = \{T:V \to V: T \text{ has at least one eigenvector} \}$$. Without loss of generality, let $\vv_1$ be an eigenvector for all $T \in V'$. Let $\{T_1, \dots, T_{(n-1)^2}\}$ be a basis of $\Lagr(\spn\vv_1^{\perp})$. Define $\{S_1, \dots, S_{(n-1)^2}\}$ such that $$S_i \sum_{j = 1}^n \alpha_j \vv_j = \alpha_1 \vw_1 + T_1 \sum_{i=2}^n \alpha_j \vv_j$$In other words, $S_i\vv_1 = \vw_1$ and for any other $\vv$, $S_i\vv = T_i\vv$ projected into $V$. $\{S_1, \dots, S_{(n-1)^2}\}$ is a basis of $V'$ since all of its elements are linearly independent (due to the linear independence of $\{T_1, \dots, T_{(n-1)^2}\}$) and it can sum to any linear map with at least one eigen vector. $\{S_1, \dots, S_{(n-1)^2}\}$ can be extended with $\{S_{(n-1)^2 + 1}, \dots, S_{n^2}\}$ to a basis of $\Lagr(V)$. It follows that $\spn \{S_{(n-1)^2 + 1}, \dots, S_{n^2}\}$ are the set of all maps without an eigen vector. Then by a) $\spn \{S_{(n-1)^2 + 1}, \dots, S_{n^2}\} = V_-$, so $V_-$ has dimension $n^2 - (n-1)^2 = 2n-1$.
        \end{enumerate}
    \end{solution}

    \begin{customex}{\textbf{10}}
        $V$ a finite dimensional vector space of $\dim(V) = n$ with an inner product.
        \begin{enumerate}[label = \alph*)]
            \item Show that if $T: V \to V$ has $k$ nonzero vectors $v_1, \cdots, v_k$ so that $T(v_i) = \lambda_iv_i$ and $\lambda_i \neq \lambda_j$ if $i \neq j$, then $\{\vv_1, \cdots, \vv_k\}$ are all linearly independent.
            \item Show that if $\lambda$ is an eigenvalue of $T: V \to V$, then it is also an eigenvalue of $T^*$.
            \item If $v$ is an eigenvector of $T : V\to V$ , is it also an eigenvector of $T^*$? Prove or give a counterexample.
            \item Assuming $n \ge 2$, find $T: V \to V$ so that $T^2 = \vzero$ but $T \neq \vzero$.
            \item Show that if $T : V \to V$ linear then $\ker(T^* \circ T) = \ker T$.
            \item Conclude that if $T$ is self-adjoint, then $T^k = \vzero$ implies $T = \vzero$, for any $k \in \N$.
            \item Show that if $T$ is diagonalizable, i.e., $V$ has an orthonormal basis made from eigenvectors of $T$, then $T$ is self-adjoint.
        \end{enumerate}
    \end{customex}

    \begin{solution}
        \begin{enumerate}[label = \alph*)]
            \item Clearly if $T$ has $1$ nonzero vector $\vv_1$, it is linearly independent. Suppose there exists some greatest $n \in \N$ such that $T$ has $n$ nonzero vectors $v_1, \cdots, v_n$ so that $T(v_i) = \lambda_iv_i$ and $\lambda_i \neq \lambda_j$ if $i \neq j$, then $\{\vv_1, \cdots, \vv_n\}$ are all linearly independent. Then for $n + 1$ with vectors $v_1, \cdots, v_n$ and corresponding eigenvalues there exists $\alpha_1, \cdots, \alpha_n \neq 0$ such that $\sum_{i = 1}^j \alpha_i \vv_i = \vv_{n+1}$. Then by applying $T$, we have $$\sum_{i = 1}^j \alpha_i \lambda_i\vv_i = \lambda_{n+1}\vv_{n+1} = \lambda_{n+1} \sum_{i=1}^n \alpha_i \vv_i = \sum_{i=1}^n \lambda_{n+1}\alpha_i \vv_i$$Since $\sum_{i = 1}^n \alpha_i \lambda_i\vv_i = \sum_{i=1}^n \lambda_{n+1}\alpha_i \vv_i$, so $\sum_{i = 1}^n \alpha_i (\lambda_i - \lambda_{n+1}) \vv_i = \vzero$. For some $1 \le j \le n$, $\alpha_j \neq 0$ but since $\{\vv_1, \cdot, \vv_n\}$ are linearly independent, $\alpha_1 (\lambda_1 - \lambda_{n+1}) = \cdots = \alpha_n (\lambda_n - \lambda_{n+1}) = 0$, so $\alpha_j (\lambda_j - \lambda_{n+1}) = 0$ and thus $\lambda_j - \lambda_{n+1} = 0$, so $\lambda_j = \lambda_{n+1}$ which is a contradiction.
            \item First we will prove that for any linear operator $T: V \to V$ and any $\lambda \in \R$, $T- \lambda I$ is not invertible if and only if $\lambda$ is an eigenvalue of $T$. Suppose $\lambda$ is an eigenvalue with eigenvector $\vv \neq \vzero$. Then $(T-\lambda I)(\vv) = \lambda \vv - \lambda \vv = \vzero$, so since $T\vzero = \vzero$, $T$ is not injective and hence not inevertible. Suppose $(T - \lambda I)$ is not invertible. Then since $T$ is a linear operator, $\nul (T - \lambda I) < \dim V$, so there exists $\vv \neq 0 \in \ker (T - \lambda I)$. Then $(T - \lambda I)\vv = T\vv - \lambda \vv = \vzero$, so $\vv$ is an eigenvector with eigenvalue $\lambda$. Next we will prove that if $(T-\lambda I)$ is not invertible then $(T-\lambda I)^*$ is not invertible. Suppose $(T-\lambda I)$ is not invertible and $(T-\lambda I)^*$ is invertible. Then let $S = ((T-\lambda I)^*)^{-1}$. It follows that $S^*$ is the inverse of $(T-\lambda I)^** = (T-\lambda I)$, which is a contradiction. Thus if $(T-\lambda I)$ is not invertible then $(T-\lambda I)^*$ is not invertible. If $\lambda$ is an eigenvalue of $T$ then $T - \lambda I$ is not invertible, so $(T - \lambda I)^* = T^* - (\lambda I)^* = T^* - \lambda I$ ($\vv\cdot (\lambda I)\vw = \lambda \vv \cdot \vw = \vw (\lambda I) \vv$, so $(\lambda I)^* = \lambda I$) is not invertible and thus $\lambda$ is an eigenvalue of $T^*$.
            \item Consider the linear operator over $\R^2$ $T(a,b) = (a+b, b)$. First I will show that $S(c,d) = (c, c+d)$ with the canonical inner product is the adjoint of $T$. For $(a,b), (c,d) \in \R^2$, $(c,d) \cdot T(a,b) = c(a+b) + db = ca + (c+d)b = (a,b) \cdot S(c,d)$. Thus $T^*(c,d) = (c, c+d)$. $T(1,0) = (1,0)$, so $(1,0)$ is an eigenvector of $T$ but $T^*(1,0) = (1,1)$ so $(1,0)$ is not an eigenvector.
            \item Let $\{\vv_1, \dots, \vv_n\}$ be a basis of $V$ and define $T\sum_{i=1}^n \alpha_i \vv_i = \alpha_1 \vv_n$. $T\sum_{i=1}^n \vv_i = \vv_n \neq \vzero$ so $T \neq \vzero$. However, $T\circ T(\sum_{i=1}^n \alpha_i \vv_i) = T(\alpha_1 \vv_n) = \vzero$.
            \item By 2.e), if $\vv \neq \vzero \in \im T$ then $\vv \in \ker(T^*)^{\perp}$, so $\vv \not \in \ker(T^*)$. Suppose $\vv \in \ker T$. Then $T\vv = \vzero$ so $T^* \circ T\vv = \vzero$, leaving $\vv \in \ker T^* \circ T$. Suppose $\vv \in \ker T^* \circ T$. Then either $T\vv = \vzero$, in which case $\vv \in \ker T$ or $T^* \circ T\vv = \vzero$ but $T\vv \neq \vzero$. Then $T\vv \in \im T$ and $T\vv \in \ker(T^*)$, which is a contradiction. Therefore, if $\vv \in \ker(T^* \circ T)$ then $\vv \in \ker(T)$. It follows that $\ker(T^* \circ T) = \ker(T)$.
            \item Let $T$ self-adjoint, and $P(n), n \in \N$ be the propisition that $(T^*)^n T(\vv) = \vzero$ implies $T\vv = \vzero$. By the previous exercise, $P(1)$ holds. Suppose for some $k \in \N$ that $P(k)$ holds. Then $(T^*)^{n+1} T\vv = T^* ((T^*)^n T\vv)$ then either $(T^*)^n T\vv = \vzero$ or $(T^*)^n T\vv \neq \vzero$ but $T^* ((T^*)^n T\vv) = \vzero$. The first case, by $P(n)$, implies that $T\vv = \vzero$. In the second case, since $T$ is self-adjoint, $T^* = T$, so $(T^*)^n T\vv = T^{n+1}\vv$, so $T^{n+1} \vv \in \im T$. Then by the same argument as in the previous exercise, a contradiction is reached. Thus the first case is the only possibility, so $T\vv = \vzero$. 
            \item Suppose $\{\vv_1, \dots, \vv_n\}$ is an orthonormal eigenvector basis of $V$ with corresponding eigenvalues $\{\lambda_1, \dots, \lambda_n\}$. Fix $\vv = \sum_{i=1}^n \alpha_i \vv_i$ and $\vw = \sum_{i=1}^n \beta_i \vv_i$. Then 
            \begin{align*}
                \vw \cdot T\vv &= \sum_{i=1}^n \alpha_i \vv_i \cdot T\sum_{i=1}^n \beta_i \vv_i\\
                &= \sum_{i=1}^n \alpha_i \vv_i \cdot \sum_{i=1}^n \beta_i \lambda_i \vv_i\\
                &= \sum_{i=1}^n \alpha_i\beta_i \lambda_i
            \end{align*}
            And similarly
            \begin{align*}
                \vv \cdot T\vv &= \sum_{i=1}^n \beta_i \vv_i \cdot T\sum_{i=1}^n \alpha_i \vv_i\\
                &= \sum_{i=1}^n \beta_i \vv_i \cdot \sum_{i=1}^n \alpha_i \lambda_i \vv_i\\
                &= \sum_{i=1}^n \beta_i\alpha_i \lambda_i
            \end{align*}
            Hence $\vw \cdot T\vv = \vv \cdot T\vw$, so $T$ is self-adjoint.
        \end{enumerate}
    \end{solution}
\end{document}

